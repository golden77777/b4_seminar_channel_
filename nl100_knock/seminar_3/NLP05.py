# -*- coding: utf-8 -*-
"""NLP05.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y6HRczbE20kC-ibDjAXdAoc9R4dUKg86
"""

#初期設定 田嶋
import CaboCha

with open("neko.txt", encoding='utf-8') as data_file:
    with open("neko.txt.cabocha", 'w', encoding='utf-8') as out_file:
        cabocha = CaboCha.Parser()
        for line in data_file:
            out_file.write(
                cabocha.parse(line).toString(CaboCha.FORMAT_LATTICE)
            )

#40 田嶋
class Morph:
    #初期化
    def __init__(self, surface, base, pos, pos1):
        self.surface = surface
        self.base = base
        self.pos = pos
        self.pos1 = pos1
        
    def __str__(self):
        return f"surface[{self.surface}], base:[{self.base}], pos:[{self.pos}], pos1:[{self.pos1}]"
        
results = []
morphs = []
with open("neko.txt.cabocha", encoding='utf-8') as f:
    #最後の1行は不要なのでスライス
    for line in f.read().split('\n')[:-1]:
        #1文ずつ追加
        if line == 'EOS':
            results.append(morphs)
            morphs = []
        #1文字目が*のときは解析結果なので除く
        elif line[0] == '*':
            continue
        else:
            #surfaceだけ
            ls = line.split('\t')
            tmp = ls[1].split(',')
            #Morphオブジェクト作成
            morphs.append(Morph(ls[0], tmp[6], tmp[0], tmp[1]))
            
for line in results[2]:
    print(line)

#41 岸本

class Chunk(object):
    def __init__(self,cont):
        self.morphs = []
        self.dst = int(cont.split(" ")[2].rstrip("D")) #係り先番号
        self.srcs = []

def read_cabocha_file(file): #CaboChaファイルを読み込んで、１文ごとにChunkオブジェクトのリストを返す
    with codecs.open(file,"r","utf-8","ignore") as f:
        sentence = []
        c = None
        for line in f:
            line = line.strip("\n")
            if line == "EOS": #１文の終わり
                for i,c in enumerate(sentence): #１行のChunkオブジェクトごとに
                    if c.dst != -1: #係り先がある場合なら
                        sentence[c.dst].srcs.append(i) #係り先のsrcsに係り元番号iを記録する
                yield sentence #１文ごとにChunkオブジェクトのリストを返す
                sentence = [] #初期化
            elif re.match(r'^\*\s\d+\s(-?\d+).*?$',line): #係り受け解析結果の行
                c = Chunk(line)
                sentence.append(c) #係り受け解析結果の行のChunkオブジェクトをsentenceに入れる
            else: #形態素解析の行
                c.morphs.append(Morph(line)) #形態素解析結果を「係り受け解析結果の行のChunkオブジェクト」のmorphs要素に入れる

sentences = list(read_cabocha_file('neko.txt.cabocha'))
for i,c in enumerate(sentences[7]): #8文目
    string = "".join([m.surface for m in c.morphs]) #文字列
    if c.dst != -1:
        target = "".join([m.surface for m in sentences[7][c.dst].morphs]) #係り先の文字列、c=sentences[7][c.dst]としている
    else:
        target = "" #係り先なし
    print("[文字列]{}\t[係り先]{}".format(string,target))

#42 岸本

for index in range(len(sentences)):
    for i,c in enumerate(sentences[index]):
        string = "".join([m.surface for m in c.morphs if not m.pos == "記号"]) #文字列 #記号を除去
        if c.dst != -1:
            target = "".join([m.surface for m in sentences[index][c.dst].morphs if not m.pos == "記号"]) #係り先の文字列、c=sentences[index][c.dst]としている
        else:
            target = "" #係り先なし
        print("[係り元]{}\t[係り先]{}".format(string,target))

# 43　下川
out_file = open("result_43.txt", "w")
# 名詞を含むかの辞書
noun_dict = dict()
# 名詞を含むかのflag
noun_flag = False
# 動詞を含むかの辞書
verb_dict = dict()
# 動詞を含むかのflag
verb_flag = False

for line in chunk_list:
    
    # 係り元(key)と係り先(value)を格納する辞書
    dst_dict = dict()
    # 文節ごとのリスト
    dst_list = []
    
    # 文章ごとにみる。
    for chunk in line:
        # textの初期化
        text = ""
        
        # 文節の取得
        for word in chunk.morphs:
            if word.surface == "。":
                continue
            text += word.surface
            # 名詞があるかどうか
            if word.pos == "名詞":
                noun_flag = True
            if word.pos == "動詞":
                verb_flag = True
                
        # textをkeyに係り先をvalueに格納
        dst_dict[text] = chunk.dst
        dst_list.append(text)
        # 名詞があるかどうか
        noun_dict[text] = noun_flag
        noun_flag = False
        # 動詞があるかどうか
        verb_dict[text] = verb_flag
        verb_flag = False
        
    for key, value in dst_dict.items():
        # 文末でなく、名詞を含む文節が動詞を含む文節に係るものを抽出
        if value != -1\
        and noun_dict[key] == True\
        and verb_dict[dst_list[value]] == True:
            out_file.write("{}\t{}".format(key, dst_list[value]))
    
    noun_dict = dict()
    verb_dict = dict()
out_file.close()

#林
def print_pair(chunk,sentence):#chunkの係元と係先を出力する関数(引数は係元の関数とその元の文のchunkのlist)
    import re
    print(re.sub("。|、","",chunk.morphs_surface())+"\t"+re.sub("。|、","",sentence[int(chunk.dst)].morphs_surface()))#。と、を削除

for sentence in chunk_list[:10]:
    for chunk in sentence:
        if chunk.dst != -1:
            if "名詞" in [morph.pos for morph in chunk.morphs] and "動詞" in [morph.pos for morph in sentence[chunk.dst].morphs]:
                print_pair(chunk,sentence)

# Commented out IPython magic to ensure Python compatibility.
#44　下川
import pydot_ng as pydot
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

# 何行目をみるか
line_num = 4
dot_pair = []

for i, line in enumerate(chunk_list):
    if i == line_num:
        
        # 係り元(key)と係り先(value)を格納する辞書
        dst_dict = dict()
        # 文節ごとのリスト
        dst_list = []
        # 文章ごとにみる。
        for chunk in line:
            # textの初期化
            text = ""
            # 文節の取得
            for word in chunk.morphs:
                if word.surface == "。":
                    continue
                text += word.surface
            # textをkeyに係り先をvalueに格納
            dst_dict[text] = chunk.dst
            dst_list.append(text)
        for key, value in dst_dict.items():
            if value != -1:
                dot_pair.append([key, dst_list[value]])
        break

graph=  pydot.graph_from_edges(dot_pair, directed=True)
graph.write_png("result44.png")
img = Image.open("result44.png")
plt.imshow(np.asarray(img))

# 岸本

from IPython.display import Image, display_png
display_png(Image('result44.png')) #表示

#45 井原
def search(morphs):
    for morph in chunk.morphs:
        if morph.pos=="動詞":
            dousi=morph.base
            id_list=chunk.srcs
            return(dousi,id_list)
    return(None,None)

with open("./45.txt","w",encoding="UTF-8")as f45:
    for chunks in result:
        for chunk in chunks:
            id_list=list()

            #動詞が文節にあるかを探索する
            dousi,id_list=search(chunk.morphs)

            #動詞が存在するときに助詞を探索して表示
            if not id_list==None:
                zyosi_list=list()
                for i in id_list:
                    for morph in chunks[i].morphs:
                        if morph.pos=="助詞":
                            zyosi_list.append(morph.surface)
                if len(zyosi_list)!=0:
                    print(dousi,end="\t")
                    print(" ".join(zyosi_list))
                    f45.write("{}\t{}\n".format(dousi," ".join(zyosi_list)))

#
#sort 45.txt | uniq -c | sort -n -r > "45_1.txt"
#grep "^する\s" 45.txt | sort | uniq -c | sort -n -r > "45_2.txt"
#grep "^見る\s" 45.txt | sort | uniq -c | sort -n -r > "45_3.txt"
#grep "^与える\s" 45.txt | sort | uniq -c | sort -n -r > "45_4.txt"

#田嶋　品詞判定の関数（Chunkクラスの中）
def chk_pos(self, pos):
        for morph in self.morphs:
            if morph.pos == pos:
                return True
        return False

#46 井原
def search(morphs):
    for morph in chunk.morphs:
        if morph.pos=="動詞":
            dousi=morph.base
            id_list=chunk.srcs
            return(dousi,id_list)
    return(None,None)


for chunks in result:
    for chunk in chunks:
        id_list=list()

        #動詞が文節にあるかを探索する
        dousi,id_list=search(chunk.morphs)

        #動詞が存在するときに係り元で助詞を探索してあれば表示
        if not id_list==None:
            surface_list=list()
            zyosi_list=list()
            for i in id_list:
                flag=0
                surface=str()
                for morph in chunks[i].morphs:
                    surface+=morph.surface
                    if morph.pos=="助詞":
                        flag=1
                        zyosi_list.append(morph.surface)
                if flag==1:
                    surface_list.append(surface)
            if len(zyosi_list)!=0:
                print(dousi,end="\t")
                print(" ".join(zyosi_list),end="\t")
                print(" ".join(surface_list))

#47林
all = list()#全体のリスト
for sentence in chunk_list:
    for chunk in sentence:
        if "サ変接続" in [morph.pos1 for morph in chunk.morphs] and "を" in [morph.surface for morph in chunk.morphs] and chunk.dst != -1:
            verb_base_list = [morph.base for morph in sentence[chunk.dst].morphs if morph.pos == "動詞"]#動詞の基本形のリスト
            if len(verb_base_list) != 0:#もし動詞があるなら
                verb_base = [morph.surface for morph in chunk.morphs if morph.pos1 == "サ変接続"][-1]+"を" + verb_base_list[0]#サ変接続+を+動詞原型
                cases = {}
                for srcs in sentence[chunk.dst].srcs:
                    for base in [morph.base for morph in sentence[srcs].morphs if morph.pos == "助詞"][-1:]:
                        if base not in cases.keys() and srcs != chunk.id :#先に重複をなくす&サ変名詞のchunkが入るのを防ぐ
                            cases[base] = sentence[srcs].morphs_surface()#{助詞:項}を辞書に加える
                sorted_cases = sorted(cases.items())#辞書順にソート
                if len(sorted_cases) > 0:#辞書の中身があったら
                    all.append(verb_base+"\t"+" ".join([s[0] for s in sorted_cases])+"\t"+" ".join([s[1] for s in sorted_cases]))

#一部出力してみる
for a in all[:10]:
    print(a)

with open("neko.cases3.txt","w",encoding="UTF-8") as f:
    f.write("\n".join(all))

#47コーパス中で頻出する述語
! cut -f 1 neko.cases3.txt | sort | uniq -c | sort -nrk1

#コーパス中で頻出する述語と助詞の関係
! cut -f 1-2 neko.cases3.txt | sort | uniq -c | sort -nrk1

#48林
def path(chunk):#chunkのパスをリスト化
    c = chunk
    path_list = [c.morphs_surface()]
    while c.dst != -1:
        path_list.append(sentence[c.dst].morphs_surface())
        c = sentence[c.dst]
    return(path_list)


sentence = chunk_list[5]
for sentence in chunk_list[:10]:
    all_path_list = []
    for chunk in sentence:
        if "名詞" in [morph.pos for morph in chunk.morphs]:#名詞が含まれていたら
            all_path_list.append(path(chunk))
    for p in all_path_list:
        print(" -> ".join(p))

#49 ひがさ
import copy
class Chunk:
    def __init__(self, morphs, dst, srcs):
        self.morphs = morphs
        self.dst = int(dst.rstrip('D'))
        self.srcs = int(srcs)

    def __str__(self) :
        return 'srcs: {}, dst: {}, morphs: ({})'\
    .format(self.srcs, self.dst, ' / '.join([str(morph) for morph in self.morphs]))

    def join_morphs(self):
        return ''.join([morph.surface for morph in self.morphs if morph.pos !='記号'])
 
    def pair(self, sentence) :
        return self.join_morphs() + '\t' + sentence[self.dst].join_morphs()

    def judge_noun(self) :
        return any([morph.pos == '名詞' for morph in self.morphs])

    def judge_verb(self) :
        return any([morph.pos == '動詞' for morph in self.morphs])

    def judge_particle(self):
        return any([morph.pos == '助詞' for morph in self.morphs])
    
    def left_verb(self):
        verb = ''
        for i in range(len(self.morphs)):
            if self.morphs[i].pos == '動詞':
                verb = self.morphs[i].base
                break
        return(verb)
    
    def extract_particle(self):
        particle = []
        for i in range(len(self.morphs)):
            if self.morphs[i].pos == '助詞':
                particle.append(self.morphs[i].base)
        return(particle)
#============================================================
    def replace_noun(self,alt):
        result = ''
        for morph in self.morphs:
            if morph.pos != '記号': 
                if morph.pos == '名詞':
                    result += alt
                    alt = ''
                else:
                    result += morph.surface
        return result


chunked_sentences = make_chunk_list('neko.txt.cabocha')    

for sentence in chunked_sentences:
    # 名詞句ペアのリスト
    #一文中の名詞のペアを抽出
    noun_index = [i for i in range(len(sentence))\
                  if sentence[i].judge_noun()]
    if len(noun_index) <2:
        continue
        
    for i ,index_x in enumerate(noun_index[:-1]):
        for index_y in noun_index[i+1:]:
            bup_y = False
            index_dup = -1
            routes_x  = set()
                
            dst = sentence[index_x].dst
            while dst != -1:
                if dst == index_y:
                    bup_y = True
                    break
                routes_x.add(dst)
                dst = sentence[dst].dst
            #ぶつからなかった場合
            if not bup_y:
                dst =sentence[index_y].dst
                while dst != -1:
                    if dst in routes_x:
                        index_dup = dst
                        break
                    else:
                        dst = sentence[dst].dst
            #結果の表示
            if index_dup == -1:
                result = []
                #sentence[index_x]の名詞をXに変更する
                x = sentence[index_x].replace_noun('X')
                result.append(x)
                dst = sentence[index_x].dst
                while dst != -1:
                    #ぶつかった位置
                    if dst == index_y:
                        result.append('->')
                        y = sentence[index_x].replace_noun('Y')
                        result.append(y)
                        break
                    else:
                        result.append('->')
                        result.append(sentence[dst].join_morphs())
                    dst = sentence[dst].dst
                print(''.join(result))
       
            else:
                result = []
                #Xからぶつかる手前までを出力
                #sentence[index_x]の名詞をXに変更する
                x = sentence[index_x].replace_noun('X')
                result.append(x)
                dst = sentence[index_x].dst
                while dst != index_dup:
                    result.append('->')
                    result.append(sentence[dst].join_morphs())
                    dst = sentence[dst].dst
                result.append('|')
                #Yからぶつかる手前までを出力
                #sentence[index_y]の名詞をYに変更する
                y = sentence[index_x].replace_noun('Y')
                result.append(y)
                dst = sentence[index_y].dst
                while dst != index_dup:
                    result.append('->')
                    result.append(sentence[dst].join_morphs())
                    dst = sentence[dst].dst
                result.append('|')
                result.append(sentence[index_dup].join_morphs())
                print(''.join(result))


