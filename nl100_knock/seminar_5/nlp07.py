# -*- coding: utf-8 -*-
"""NLP07.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_O7GYn1ByEAgbKnffgPxKgm4ipXO9nXV
"""

#順番決め
import random
member = ["井原","岸本","下川","田嶋","林","日笠"]
random.shuffle(member)
print(member)

#60 井原
from gensim.models import KeyedVectors

file_name = "./GoogleNews-vectors-negative300.bin"
model = KeyedVectors.load_word2vec_format(file_name,binary=True)
print(model['United_States'])

#61 井原
print(model.similarity("United_States","U.S."))

#62 田嶋
import pandas as pd
import numpy as np
#コサイン類似度の高い順10語
similarities = model.most_similar("United_States")
pd.DataFrame(
    similarities,
    columns = ['単語', '類似度'],
    index = np.arange(len(simularities)) + 1
)

#63 田嶋
similarities = model.most_similar(positive=['Spain', 'Athens'], negative=['Madrid'])
pd.DataFrame(
    similarities,
    columns = ['単語', '類似度'],
    index = np.arange(len(simularities)) + 1
)

#64 岸本

a = 0

#カテゴリの確認

with open("questions-words.txt","r") as f:
    for i, line in enumerate(f):
        a += 1
        if len(line.split(" ")) <= 3:
            print(line)
    print(a)

""": capital-common-countries

: capital-world

: currency

: city-in-state

: family

: gram1-adjective-to-adverb

: gram2-opposite

: gram3-comparative

: gram4-superlative

: gram5-present-participle

: gram6-nationality-adjective

: gram7-past-tense

: gram8-plural

: gram9-plural-verbs

19558
"""

from tqdm import tqdm_notebook as tqdm

lst = []

#データを読み込み、most_similar()の計算を行った上で1位の単語のみを取り出している
#もっと早い方法がありそう → topn = 1とおけばよかった ☞　変わらない

with open("questions-words.txt","r") as f:
    lines = f.read().splitlines()

for i, line in enumerate(tqdm(lines)):
    if len(line.split(" ")) >= 4:
        words = [w for w in line.split(" ")]
        ms = model.most_similar(positive=[words[1], words[2]], negative=[words[0]])
        lst.append([words[0],words[1],words[2],ms[0][0],ms[0][1]])

print(lst[:10])

"""[['Athens', 'Greece', 'Baghdad', 'Iraqi', 0.6351871490478516], ['Athens', 'Greece', 'Bangkok', 'Thailand', 0.7137669324874878], ['Athens', 'Greece', 'Beijing', 'China', 0.7235777974128723], ['Athens', 'Greece', 'Berlin', 'Germany', 0.6734622716903687], ['Athens', 'Greece', 'Bern', 'Switzerland', 0.4919748306274414], ['Athens', 'Greece', 'Cairo', 'Egypt', 0.7527809739112854], ['Athens', 'Greece', 'Canberra', 'Australia', 0.583732545375824], ['Athens', 'Greece', 'Hanoi', 'Viet_Nam', 0.6276341676712036], ['Athens', 'Greece', 'Havana', 'Cuba', 0.6460991501808167], ['Athens', 'Greece', 'Helsinki', 'Finland', 0.6899983286857605]]"""

df = pd.DataFrame(lst)
df.columns = ["1列目の単語","2列目の単語","3列目の単語","求めた単語","コサイン類似度"]
df

#65 岸本

#行数のチェック

for i, line in enumerate(lines):
    if ":" in line:
        print(i,line)

"""<pre>
0 : capital-common-countries
507 : capital-world
5032 : currency
5899 : city-in-state
8367 : family
8874 : gram1-adjective-to-adverb
9867 : gram2-opposite
10680 : gram3-comparative
12013 : gram4-superlative
13136 : gram5-present-participle
14193 : gram6-nationality-adjective
15793 : gram7-past-tense
17354 : gram8-plural
18687 : gram9-plural-verbs
</pre>

gramから始まる章の単語アナロジーの評価データは文法的アナロジーに該当(index:0〜8873)

それ以外は意味的アナロジーに該当(index:8874〜19557)
"""

semantic_analogy_true = []
syntactic_analogy_true = []

#真の値を格納
for i, line in enumerate(lines):
    if not ":" in line:
        if i <= 8873:
            semantic_analogy_true.append(line.split(" ")[3])
        else:
            syntactic_analogy_true.append(line.split(" ")[3])
            
print(len(semantic_analogy_true))
print(len(syntactic_analogy_true))

"""8869

10675
"""

# 66 下川
with open("wordsim353/combined.csv") as file:
    combined = file.readlines()

combined_vector = []
for i, word in enumerate(combined):
    word = word.split(",")
    if word[0] == "Word 1":
        continue
    simularity = model.similarity(word[0], word[1])
    combined_vector.append(word + [simularity])

# 人間による類似度のみ取り出す
human_sim = [word[2] for word in combined_vector]
# 単語ベクトルによる類似度のみを取り出す
vector_sim = [word[3] for word in combined_vector]

import numpy as np
from scipy.stats import rankdata, spearmanr

# ランキングにする
human_rank = rankdata(np.array(human_sim))
vector_rank = rankdata(np.array(vector_sim))

spearman, p = spearmanr(human_rank, vector_rank)

print('順位相関係数 : {}'.format(spearman))
print('p値 : {}'.format(p))

# 67 下川
country_set = set()
for word in word_list:
    if word[0] == "capital-common-countries":
        country_set.add(word[2])
        country_set.add(word[4])
    elif word[0] == "capital-world":
        country_set.add(word[2])
        country_set.add(word[4])
    elif word[0] == "currency":
        country_set.add(word[1])
        country_set.add(word[3])

country_list = list(country_set) 
# 国名をベクトル化
country_vectors = [model[country] for country in country_list]

# kmeans
from sklearn.cluster import KMeans

cluster_size = 5

kmeans = KMeans(n_clusters=cluster_size, random_state=0)
kmeans.fit(country_vectors)

country_cluster = [[],[],[],[],[]]

for i in range(len(country_list)):
    country_cluster[kmeans.labels_[i]].append(country_list[i])

for i in range(cluster_size):
    print('クラス : {}'.format(i))
    print(', '.join([country for country in country_cluster[i]]))

# Commented out IPython magic to ensure Python compatibility.
#68 林
import matplotlib.pyplot as plt
# %matplotlib inline
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster

linkage_result = linkage(country_vec, method='ward', metric='euclidean')
# 階層型クラスタリングの可視化
plt.figure(num=None, figsize=(16, 9), dpi=200, facecolor='w', edgecolor='r')
dendrogram(linkage_result,labels = country_list,color_threshold=1)
plt.show()

#69 日笠
import scipy as sp
from sklearn import manifold
import matplotlib.pyplot as plt

with open('country.txt') as f:
    lines = f.read().splitlines()

x = []
y = []
for line in lines:
    try:
        x.append(model[line])
        y.append(line)
    except:
        pass

#２次元に圧縮    
t_sne = manifold.TSNE(n_components=2)
vect = t_sne.fit_transform(x)

#転置させる
vect_t = list(zip(*vect)) 
#関数の引数に*をつけるとリストを展開して渡せるという仕組みを使う。

#annotateをつけるためsubplotする
fig, ax = plt.subplots(figsize=(18, 15))

plt.scatter(vect_t[0],vect_t[1])
#annotateをつける
for i, c in enumerate(y):
    ax.annotate(c, (vect[i][0],vect[i][1]))
